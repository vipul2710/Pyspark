{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c76e4bc-ed4c-448b-affa-329ef5f4066b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
    "\n",
    "data = [\n",
    " (\"James\", \"A\", \"Smith\", 3000),\n",
    " (\"Michael\", \"B\", \"Rose\", 4000),\n",
    " (\"Robert\", \"C\", \"Williams\", 2500),\n",
    " (\"Maria\", \"D\", \"Jones\", 5000)\n",
    "]\n",
    "\n",
    "schema = StructType([StructField(\"first_name\", StringType(), True),\n",
    "                     StructField(\"middle_name\", StringType(), True),\n",
    "                     StructField(\"surname\", StringType(), True),\n",
    "                     StructField(\"salary\", IntegerType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dc5ebaf-6098-4fed-a7ac-97d75bac7bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    " (\"James\", \"A\", \"Smith\", \"NY\", 3000),\n",
    " (\"Michael\", \"B\", \"Rose\", \"CA\", 4000),\n",
    " (\"Robert\", \"C\", \"Williams\", \"TX\", 2500),\n",
    " (\"Maria\", \"D\", \"Jones\", \"CA\", 5000),\n",
    " (\"Jen\", \"\", \"Brown\", \"NY\", 0)\n",
    "]\n",
    "\n",
    "cols = [\"first\", \"middle\", \"last\", \"state\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5067b248-674e-4cbf-8a77-b69a762dbc3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.select(\"first\",\"last\",\"state\")\n",
    "df2.show()\n",
    "from pyspark.sql.functions import col\n",
    "df3 = df.filter(col(\"salary\")>3000)\n",
    "df3.show()\n",
    "\n",
    "df4 = df.filter((col(\"state\") == \"CA\") | (col(\"state\") == \"NY\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e881f7-7581-4992-a516-3751ec7993a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a new column salary_k = salary / 1000\n",
    "df1 = df.withColumn(\"salary_k\",col(\"salary\")/1000).show(truncate=False)\n",
    "\n",
    "#Create a new column full_name = first + \" \" + last\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "\n",
    "df_1 =df.withColumn(\"full_name\", concat(col(\"first\"), lit(\" \"), col(\"last\")))\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7355ef7e-ed0f-4427-85ec-204619dedc58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sort by salary descending\n",
    "df.sort(col(\"salary\").desc()).show(truncate=False)\n",
    "#Group by state and find the average salary\n",
    "\n",
    "df_sorted = df.orderBy(col(\"salary\").desc())\n",
    "df_top_per_state = df_sorted.dropDuplicates([\"state\"])\n",
    "df_top_per_state.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc355bb-6dd4-4716-990a-d5b582953669",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"state\").avg(\"salary\").show(truncate=False)\n",
    "\n",
    "#total_employers\n",
    "df.groupBy(\"state\").count().show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d7e4f0a-d18d-482c-b147-108c0b570014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state_dim = [\n",
    "  (\"NY\", \"East Coast\"),\n",
    "  (\"CA\", \"West Coast\"),\n",
    "  (\"TX\", \"South Central\")\n",
    "]\n",
    "\n",
    "df_dim = spark.createDataFrame(state_dim, [\"state\", \"region\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04606e38-e3df-4a9b-b571-137dec62e467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_inner = df.join(df_dim, df.state == df_dim.state, \"inner\")\n",
    "df_inner.show(truncate=False)\n",
    "df_left = df.join(df_dim, df.state == df_dim.state, \"left\")\n",
    "df_left.show(truncate=False)\n",
    "df_leftan = df.join(df_dim, df.state == df_dim.state, \"leftanti\")\n",
    "df_leftan.show(truncate=False)\n",
    "print(\"Unmatched count:\", df_leftan.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1ddca5-7bb0-4760-b2a0-b372863ff048",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "practice intensive pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
